# 深層学習を学ぶためのキーワード集

深層学習を学ぶ際のとっかかりとして参照しやすいように，キーワード集を作成しました．
各箇条書きの上から順に重要そうなキーワードになっています．

もしこれもあった方がいいんじゃないかといったものがあれば教えてください．

## 基礎

- ニューラルネットワーク (Neural network)
- 誤差逆伝播法 (Backpropagation)
- 勾配 / 勾配降下法 (Gradient / Gradient decent)
- 活性化関数 (Activation function)
- 損失関数 / 目的関数 (Loss function / Objective function)
- 学習率 (lr: Learning Rate)
- バッチサイズ (Batch size)
- オプティマイザ (Optimizer)
- テンソル (Tensor)
- 潜在空間 (Latent space)

## 学習方法

- 教師あり学習 (Supervised learning)
- 教師なし学習 (Unsupervised learning)
- 半教師あり学習 (Semi-supervised learning)
- 事前学習 (Pretraining)
- 転移学習 (Transfer learning)
- ファインチューニング (Fine tuning)
- 深層距離学習 (Deep metric learning)
- 知識蒸留 (Knowledge distillation)

## 代表的なアーキテクチャ

- 多層パーセプトロン (MLP: Multi-Layer Perceptron)
- 畳み込みニューラルネットワーク (CNN: Convolutional Neural Network)
- 再帰型 / 回帰型ニューラルネットワーク (RNN: Recurrent Neural Network)
- 敵対的生成ネットワーク (GAN: Generative Adversarial Network)
- オートエンコーダ (AE: Auto Encoder)
- 変分オートエンコーダ (VAE: Variational Auto Encoder)
- トランスフォーマー (Transformer)
- 拡散モデル (Diffusion Model)
- 状態空間モデル (SSM: State-Space Model)

## 活性化関数

- ReLU (Rectangle Linear Unit)
- sigmoid
- softmax
- leaky ReLU
- tanh
- GELU (Gaussian Error Linear Unit)
- GLU (Gated Linear Unit)

## 損失関数

- MSE (Mean Square Error)
- L1 loss
- CE (Cross Entropy)
- BCE (Binary Cross Entropy)
- KL divergence loss (Kullback-Leibler divergence loss)
- CTC loss (Connectionist Temporal Classification)
- Triplet loss

## CNN

- カーネル / カーネルサイズ (Kernel / Kernel size)
- パディング (Padding)
- ストライド (Stride)
- 拡張/膨張畳み込み (Dilated convolution)
- Depthwise convolution
- Pointwise convolution

## レイヤー

- 線形層 / 全結合層 (Linear layer / Fully Connected layer)
- 畳み込み層 (Convolution layer)
- 逆畳み込み層 (Deconvolutioin layer)
- ドロップアウト層 (Dropout layer)
- Batch Normalization
- Layer Normalization
- LSTM (Long Short Term Memory, 長短期記憶)
- Bi-LSTM (Bidirectional LSTM)
- Attention / Self-Attention
- Multi-Head Self-Attention (MHSA)
- プーリング層 (Pooling layer)
- Pixel Shuffler
